---
title: 正则化
---

模型的复杂度上升会让模型能力更强，这是我们希望的。但是其副作用就是过拟合。如果为了防止过拟合就放弃复杂模型，其结果就像是倒掉洗澡水的时候把婴儿一起倒掉了。那怎么办呢？我们可以使用正则化技术在使用复杂模型的前提下防止过拟合。






来看下这个复杂的吓人的某逻辑回归公式：

$$
\sigma\left(b+w_1x_1+\ldots+w_nx_n+w_1{x_1}^2+\ldots+w_n{x_n}^2+w_{12}x_1x_2+\ldots+w_{1n}{x_1x}_n+\ldots w_{23}x_2x_3+\ldots+w_{2n}{x_2x}_n\right)
$$

能不能把它的每部分权重降低呢？就像把狼驯化为狗，我们可以使用什么办法驯化这个吓人的公式吗？正则化方法可以帮助我们完成这个任务。

逻辑回归的损失函数 
$$
L=-[ylog\hat{y}+(1-y)\log(1-\hat{y})]
$$
如果将损失函数改写为：

$$
L=-\left[ylog\hat{y}+\left(1-y\right)\log{\left(1-\hat{y}\right)}\right]+\ \lambda\sum w^2
$$

其中 $\lambda$ 是正则化参数。因为模型优化的目标是降低损失，如果 $\lambda$ 很大的话，为了尽可能地降低损失，就会尽可能地降低 $w$。这样一来，因为系数降低了，所以每一个特征（包括低阶和高阶）的影响都降低了，相当于把不受约束的力量加了锁，将狼驯化为了狗。

现在机器学习中，由于数据量的增大，人们更普遍地采取使用正则化的方法来防止模型过拟合，而不是特征选择的方法。当然，为了增强模型的可解释性，特征选择的方法仍是一个好办法。
