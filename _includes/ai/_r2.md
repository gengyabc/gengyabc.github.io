### 决定系数

假设一组数据 $X_1, X_2, …, X_n$，我告诉了你除了 $X_n$ 以外所有的数值，让你猜猜 $X_n$ 是多少，你觉得下面那个更靠谱： 

* 最大值
* 最小值
* 平均值
* 随便猜一个

相信大多数人都会觉得平均值更合适一些。如果就把这个平均值画出来，然后找到每个实际的观测点到这个平均线的距离，算出这些距离的平方和：$\Sigma{(y_i - \bar{y})^2}$，这里的 $\bar{y}$ 就是平均值。

![](/assets/images/ai/models/01/mean.png)

下面我们给出两个定义：

$$
SS_{res} = \Sigma{(y_i - \hat{y_i})^2}
$$

$$
SS_{tol} = \Sigma{(y_i - \bar{y})^2}
$$

上面第一个公式是残差平方和，判断了观测点和回归线接近程度，第二个公式是总平方和，判断了各个测量点和平均值的接近程度。下面，我们定义决定系数：

$$
R^2 = 1 - \dfrac {SS_{res}}{SS_{tol}}
$$

观察这个公式及下图，线性回归（右侧）的效果比起平均值（左侧）越好，决定系数的值就越接近于1。 蓝色正方形表示线性回归的残差的平方， 红色正方形数据表示对于平均值的残差的平方。

![](/assets/images/ai/models/01/r.png) 

### 调整的 $R^2$

$R^2$ 可以较好的反映拟合的好坏，但是有一个小问题：随着自变量的增多，$SS_{res}$ 不会变大，导致决定系数不会变小。这是否告诉我们，模型的自变量越多越好呢？事实上，模型越复杂，模型越容易出问题，简单一些的模型才好。为了能够更公平地对比不同自变量个数的模型，我们引入调整的 $R^2$（Adjusted $R^2$）：

$$
Adj \ R^2 = 1 - (1-R^2)\dfrac {n - 1}{n - p -1}
$$

其中，$p$ 是自变量数目，$n$ 是样本数目。